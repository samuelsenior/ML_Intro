{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to Practical 1.2 - Iris Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are continuing with using basic classifiers, except this time we are working with a famous external dataset: the Iris Dataset, developed by Fisher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the Iris dataset. We see that the features $X$ are a 4-column matrix with all continuous data as the sepal and petal length and width. The y labels are a number [0, 1, 2] representing the associated species with the measurements. You can look up the finer details, in addition to a full view of the entire dataset here: (https://en.wikipedia.org/wiki/Iris_flower_data_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of features: [[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]]\n",
      "First 5 rows of labels: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(\"First 5 rows of features: {}\".format(X[0:5]))\n",
    "print(\"First 5 rows of labels: {}\".format(y[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size: (150, 4)\n",
      "Label size: (150,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature size: {}\".format(X.shape))\n",
    "print(\"Label size: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "In this example we're going to select a few random indices out of 150, remove them from the dataset and use them as our test samples. The rest of the data, we will give to the classifier to train on the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nIndices = 4\n",
    "test_idx = np.random.randint(0,150,nIndices)\n",
    "# ensure the test indices are unique\n",
    "test_idx = np.unique(test_idx)\n",
    "\n",
    "train_X = np.delete(iris.data, test_idx, axis=0)\n",
    "train_y = np.delete(iris.target, test_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will formulate our test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = iris.data[test_idx]\n",
    "test_y = iris.target[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our training and testing sets, lets create and fit our classifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test our classifier on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 2]\n",
      "[0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)\n",
    "print(clf.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the examples, we see that the classifier appears to be reliably predicting the correct species, given the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "In the above example, we generate some random integers to act as the indices which select which rows to use as test samples, leaving the majority as training data for the classifier. This practice is so common in Machine Learning there is an infinity-valued function called 'train_test_split' which takes the $X$ and $y$ and returns X_train, X_test, y_train and y_test with optimised splitting. Implement the iris dataset using the train_test_split function.\n",
    "\n",
    "The package you will need is from sklearn.model_selection.\n",
    "\n",
    "Look up the function from the Ski-Kit Learn documentation and use it to generate your training and testing data. Try with 50/50 split, 75/25 and 90/10 (training/testing, respectively) and see which one has the highest accuracy.\n",
    "\n",
    "You can test which one has the highest accuracy using a quantitative scoring function called 'accuracy_score' which is already given. This can be used by giving it the y_test data and the information returned from your classifiers' predict() method. It will return a percentage, where 100% is perfect prediction with all test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy with 50/50: 0.92\n",
      "Decision Tree Accuracy with 75/25: 0.8947368421052632\n",
      "Decision Tree Accuracy with 90/10: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier = tree.DecisionTreeClassifier()\n",
    "\n",
    "# 50/50\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n",
    "classifier.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, classifier.predict(X_test))\n",
    "print(\"Decision Tree Accuracy with 50/50: {}\".format(accuracy))\n",
    "\n",
    "#75/25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "classifier.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, classifier.predict(X_test))\n",
    "print(\"Decision Tree Accuracy with 75/25: {}\".format(accuracy))\n",
    "\n",
    "#90/10\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "classifier.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, classifier.predict(X_test))\n",
    "print(\"Decision Tree Accuracy with 90/10: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy generally goes up the more training samples we have. Run it a few times to see the values change slightly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same dataset, use a different classifier than the Decision Tree. We recommend using\n",
    "the K Nearest Neighbours classifier.\n",
    "\n",
    "The import package you want for this is 'sklearn.neighbors'.\n",
    "\n",
    "Again, check out the documentation for this classifier on the website and implement it; the code is remarkably similar to the previous example. \n",
    "\n",
    "Once you have it working, compare the results to the Decision Tree classifier; is there much of a difference? Which is better? What are the advantages and disadvantages of each of the algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours Accuracy with 50/50: 0.96\n",
      "K-Nearest Neighbours Accuracy with 75/25: 1.0\n",
      "K-Nearest Neighbours Accuracy with 90/10: 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier2 = KNeighborsClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n",
    "classifier2.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, classifier2.predict(X_test))\n",
    "print(\"K-Nearest Neighbours Accuracy with 50/50: {}\".format(accuracy))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "classifier2.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, classifier2.predict(X_test))\n",
    "print(\"K-Nearest Neighbours Accuracy with 75/25: {}\".format(accuracy))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "classifier2.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, classifier2.predict(X_test))\n",
    "print(\"K-Nearest Neighbours Accuracy with 90/10: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some slightly strange output which changes every time you run it. Sometimes the lower splits are more accurate than 90/10. Figure out why!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4 - Optional\n",
    "\n",
    "Using what you have learnt from this dataset, apply it to another dataset.\n",
    "\n",
    "Go to http://scikit-learn.org/stable/datasets/ and select the load_digits() dataset as it is also a classification problem. Select the diabetes dataset if you really want a challenge!\n",
    "\n",
    "Like the iris dataset, these sets can also be retrieved through the sklearn.datasets class by one simple function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   5. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,  10.   0.   0.]\n",
      " [  0.   0.   0. ...,  16.   9.   0.]\n",
      " ..., \n",
      " [  0.   0.   1. ...,   6.   0.   0.]\n",
      " [  0.   0.   2. ...,  12.   0.   0.]\n",
      " [  0.   0.  10. ...,  12.   1.   0.]]\n",
      "[0 1 2 ..., 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn from the website that the data is represented as 64 columns, where each column is a number between [0, 16] that describes a pixel in an 8x8 grid to draw one of the 9 numerical characters. In this dataset we have 1797 total characters. This is the beginning of image classification which we will touch on in a later practical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10c7bc9b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(X.max(), X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size 0.1, Prediction 0.8333333333333334\n",
      "Test size 0.2, Prediction 0.8555555555555555\n",
      "Test size 0.3, Prediction 0.8407407407407408\n",
      "Test size 0.4, Prediction 0.8386648122392212\n",
      "Test size 0.5, Prediction 0.8175750834260289\n",
      "Test size 0.6, Prediction 0.7831325301204819\n",
      "Test size 0.7, Prediction 0.7813990461049285\n",
      "Test size 0.8, Prediction 0.8087621696801113\n",
      "Test size 0.9, Prediction 0.7200247218788628\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i/10)\n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    predictions = accuracy_score(y_test, classifier.predict(X_test))\n",
    "    print(\"Test size {}, Prediction {}\".format((i/10),predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size 0.1, Prediction 0.9944444444444445\n",
      "Test size 0.2, Prediction 0.9916666666666667\n",
      "Test size 0.3, Prediction 0.9814814814814815\n",
      "Test size 0.4, Prediction 0.9888734353268428\n",
      "Test size 0.5, Prediction 0.9810901001112347\n",
      "Test size 0.6, Prediction 0.969416126042632\n",
      "Test size 0.7, Prediction 0.9689984101748808\n",
      "Test size 0.8, Prediction 0.958970792767733\n",
      "Test size 0.9, Prediction 0.9276885043263288\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i/10)\n",
    "    classifier = KNeighborsClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    predictions = accuracy_score(y_test, classifier.predict(X_test))\n",
    "    print(\"Test size {}, Prediction {}\".format((i/10),predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the K Neighbours algorithm is much better in this example. We could also use more advanced classifiers such as Random Forests. We won't touch on these in this seminar. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
